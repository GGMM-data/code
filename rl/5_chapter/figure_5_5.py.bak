import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import time

ACTION_BACK = 0
ACTION_END = 1

class Env:
    def __init__(self):
        self.prob = 0.9

    def step(self, action):
        """
        0: left, 1: right
        """
        next_state = 0
        reward = 0.0
        if action == 0:
            # prob = np.random.rand()
            # if prob > self.prob:    # 0.1
            result = np.random.binomial(1, 0.9)     #
            if result == 0:     #
                next_state = -1
                reward = 1.0
        elif action == 1:
            next_state = -1
        return next_state, reward

        
class policy:
    def __init__(self, left_prob):
        self.left_prob = left_prob

    def action(self):
        action = np.random.binomial(1, 1 - self.left_prob)
#         prob = np.random.rand()
#         if prob < self.left_prob:
#             action = 0
#         else:
#             action = 1
        return action


def figure_5_4():
    env = Env()
    target_action_prob = 1.0
    behaviour_action_prob = 0.5
    # target_policy = policy(target_action_prob)
    behaviour_policy = policy(behaviour_action_prob)
    ratios_list = []
    returns_list = []
    values_list = []
    runs = 10
    episodes = 100000
    for i in range(runs):
        for j in range(episodes):
            returns = 0
            ratio = 1.0
            # run every episode
            trajectory = []
            while True:
                action = behaviour_policy.action()
                trajectory.append(action)
                if action == ACTION_END:
                    returns = 0
                    break
                if np.random.binomial(1, 0.9) == 0:
                    returns = 1
                    break
                # action = behaviour_policy.action()
                # next_state, reward = env.step(action)
                # returns += reward
                #
                # if action == 0:
                #     ratio = ratio * target_action_prob / behaviour_action_prob
                # elif action == 1:
                #     ratio = 0.0
                #     break
                #
                # if next_state == -1:
                #     break
            if trajectory[-1] == ACTION_END:
                ratio = 0
            else:
                ratio = np.power(2, len(trajectory))
                
            ratios_list.append(ratio)
            returns_list.append(returns)
            values_list.append(ratio * returns)
            # values_list.append(np.sum(np.multiply(ratios_list, returns_list)) * 1.0/episode_num )
        estimations = np.asarray(values_list) / np.arange(1, episodes + 1)
        plt.plot(estimations)
        
        ratios_list = []
        returns_list = []
        values_list = []
    
    # plt.ion()
    # plt.pause(0.1)
    # plt.ioff()
    plt.xlabel('Episodes (log scale)')
    plt.ylabel('Ordinary Importance Sampling')
    plt.xscale('log')

    plt.savefig('../images/figure_5_4.png')
    plt.show()
    plt.close()

if __name__ == "__main__":
    figure_5_4()
